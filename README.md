


# Bridging the Gap: Teacher-Assisted Wasserstein Knowledge Distillation for Efficient Multi-Modal Recommendation

## Introduction

This is the Pytorch implementation for our WWW 2025 paper:
> **[WWW ’25]** Ziyi Zhuang, Hanwen Du, Hui Han, Youhua Li, Junchen Fu, Joemon M. Jose, and Yongxin Ni. _Bridging the Gap: Teacher‑Assisted Wasserstein Knowledge Distillation for Efficient Multi‑Modal Recommendation_. In Proceedings of the ACM Web Conference 2025 (WWW ’25), April 28–May 2, 2025, Sydney, NSW, Australia. ACM, New York, NY, USA. 12 pages. DOI: 10.1145/3696410.3714852  
> ![image](https://github.com/user-attachments/assets/c648e43f-e5c4-4f28-b633-50648ce25073)




## Getting Started

To train the model, you can easily achieve results by running the following command:

```bash
python TARec/scripts/tiktok/train_tiktok_module_ab.sh
```


## Citation
If you find Guider useful in your research, please consider citing our [paper](https://dl.acm.org/doi/abs/10.1145/3696410.3714852).
```
@inproceedings{zhuang2025bridging,
  title={Bridging the Gap: Teacher-Assisted Wasserstein Knowledge Distillation for Efficient Multi-Modal Recommendation},
  author={Zhuang, Ziyi and Du, Hanwen and Han, Hui and Li, Youhua and Fu, Junchen and Jose, Joemon M and Ni, Yongxin},
  booktitle={THE WEB CONFERENCE 2025},
  year={2025}
}
```
This code is made available solely for academic research purposes.
